#summary Getting started guide for python mapper library

== Adding the !MapReduce Library To Your Application ==

Checkout the mapreduce folder into your application directory:

{{{
svn checkout http://appengine-mapreduce.googlecode.com/svn/trunk/python/src/mapreduce
}}}

Add the mapreduce handler to your app.yaml:

{{{
handlers:
- url: /mapreduce(/.*)?
  script: mapreduce/main.py
  login: admin
}}}

== Defining a Mapper ==

Create a function taking a single argument. It will be called for each value returned by the input reader (later in the guide, we'll discuss the type of values each input reader returns). You can do anything in a mapper that you would normally do in a task queue invocation (e.g. interact with memcache or the datastore using the normal API). However, for efficiency, you may want to look at the sections "Modifying Datastore entities in the Mapper" and "Counters" below for additional operations only available in the mapper. An example mapper for the !DatastoreInputReader might look like:

{{{
def process(entity):
  # do something with entity here.
}}}

Register the mapper in mapreduce.yaml:

{{{
mapreduce:
- name: <Some descriptive name for UI>
  mapper:
    input_reader: mapreduce.input_readers.DatastoreInputReader
    handler: <your handler name, e.g. main.process>
    params:
    - name: entity_kind
      default: <your entity name, e.g. main.MyEntity>
}}}

==Running the Mapper==

Navigate your browser to `http://<your_app_id>.appspot.com/mapreduce/`
Click the launch button to start registered mapreduce. Go to the mapreduce detail page to observe its status and control its execution.

==Modifying Datastore entities in the Mapper==
Mapper code can perform any kind of activity. But it's much more efficient to yield datastore mutation operations than to modify the datastore directly:

{{{
from mapreduce import operation as op
def process(entity):
 # change entity
 yield op.db.Put(entity)
 # or yield op.db.Delete(entity)
}}}

All operations will be batched using a mutation pool. 

==Counters==

To change counter value yield a counter operation:

{{{
from mapreduce import operation as op
def process(entity):
 yield op.counter.Increment("counter1")
 yield op.counter.Increment("counter2", 30)
 yield op.counter.Increment("counter3", -5)
}}}

==Custom mapper parameters==

The entries in the 'params' section of mapreduce.yaml are used to define per-mapper parameters. They're used by the input readers - for example, the 'entity_kind' key above is used by the default datastore reader - but they can also be used to pass arguments to your mapper, as well.

User parameters for the mapper can be accessed via the 'context' module:

{{{
from mapreduce import context

def process(entity):
  ctx = context.get()
  params = ctx.mapreduce_spec.mapper.params
  my_mapper_arg = params['my_mapper_arg']
  # Process the entity
}}}

Note that all mapper parameters are strings, so if you need an integer or other datatype, you will need to convert it in your mapper.

Additionally, the following parameters, specified directly in the mapper spec are shared by all input readers:

|| *Key* || *Default value* || *Explanation* ||
|| shard_count || 8 || The number of concurrent mapper workers to run at once. ||
|| processing_rate || 100 || The aggregate maximum number of inputs processed per second by all mappers. Can be used to avoid using up all quota, interfering with online users. ||

==Specifying readers==
The mapreduce library isn't restricted to mapping over datastore entities. It comes bundled with other input readers, defined in mapreduce/input_readers.py. Currently, this includes !DatastoreInputReader (the default), !BlobstoreLineInputReader, which maps over lines from one or more blobs in the blobstore, and !BlobstoreZipInputReader, which maps over the contents of zip files in the blobstore.

This example mapreduce.yaml demonstrates how to specify an alternate input reader:

{{{
mapreduce:
- name: Codesearch
  mapper:
    input_reader: mapreduce.input_readers.BlobstoreZipInputReader
    handler: <your handler function>
    params:
    - name: blob_key
}}}

== Provided Input Readers ==

=== !DatastoreInputReader ===

!DatastoreInputReader Reads all model instances of a particular kind from the datastore. If the keys_only parameter is false (the default), then it calls the mapper once with each model instance. If key_onlys is true, then it calls the mapper once with each key.

|| *Parameter* || *Default value* || *Explanation* ||
|| entity_kind || None || The datastore kind to map over. ||
|| batch_size || 50 || The number of entities to read from the datastore with each batch get. ||
|| keys_only || False || If True, then give the mapper only keys rather than full model instances. ||

=== !BlobstoreLineInputReader ===

!BlobstoreLineInputReader reads a \n delimited text file a line at the time. It calls the mapper once with each line, passing it a tuple comprised of the byte offset in the file of the first character in the line and the line as a string, not including the trailing newline. In other words: (byte_offset, line_value).

|| *Parameter* || *Default value* || *Explanation* ||
|| blob_keys || None || Either a string containing a blob key string or an array containing multiple blob key strings. ||

=== !BlobstoreZipInputReader ===

!BlobstoreZipInputReader iterates over all compressed files in a zipfile in Blobstore. It calls the mapper once for each file, passing it the tuple comprised of the zipfile.!ZipInfo entry for the file, and a callable that returns the complete body of the file as a string. In other words: (zipinfo, file_callable).

|| *Parameter* || *Default value* || *Explanation* ||
|| blob_key || None || A string containing a blob key. ||

==Doing per-row “reduces”==

We don’t have the reduce capability yet, but you can approximate this if you have an Entity kind with a unique constraint on a particular filter. For example, say we had these entity kinds:

{{{
class UserKind(db.Model):
  # key_name is also the ldap
  ldap = db.StringProperty(required=True)
  last_week_attendance = db.IntegerProperty(default=0)

class ClassAttendance(db.Model):
  ldap = db.StringProperty(required=True)
  when = db.DateTimeProperty(required=True)
}}}

If you know you only have a single !UserKind per LDAP, then you can Map over the !UserKind and have a map function like this:

{{{
def find_attendance(user_entity):
  last_week = (
    datetime.datetime.now() -
    datetime.timedelta(days=7))
  count = (ClassAttendance.all()
      .filter('ldap =', user_entity.ldap)
      .filter('when >=', last_week)
      .count())
  user_entity.last_week_attendance = count
  yield op.db.Put(user_entity)
}}}

Currently we do not support transactions in the map function, so the Put operations yielded here will blindly overwrite any other data in the Datastore. For now it's best to be careful and stop live traffic from accessing these entities while a background job is running. We'll be adding transactions soon.

==Validator functions==
It is also possible to specify a validator function, which will be run before starting the mapreduce, passed the user parameters specified for this mapreduce, and given the opportunity to validate and modify them.

To use a validator function, specify it in your mapreduce spec with the 'params_validator' key:

{{{
mapreduce:
- name: Codesearch
  mapper:
    handler: <your handler>
    params:
    - name: file_id
    params_validator: <your validator function>
}}}

The validator function should accept a single argument, the dict of user params, which it may modify:

{{{
def my_validator(user_params):
  file_id = user_params['file_id']
  user_params['blob_key'] = File.get_by_id(int(file_id)).blob_key
}}}

==Not Documented==

  * Error handling

== Current Limitations ==

  * Only full range scan is supported, i.e. it's impossible to scan a subset of a particular entity kind.